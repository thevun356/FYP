# -*- coding: utf-8 -*-
"""POC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhCC-JdMphmr2B8h8V38q4rEkDeCNkLV
"""

!pip show tensorflow

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Final Year Project/Datasets/Cat & Dog/train

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18, ResNet18_Weights
from torch import nn, optim
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from google.colab import drive
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib.pyplot as plt
import requests
from io import BytesIO

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Data preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load dataset from single directory
train_path = '/content/drive/MyDrive/Final Year Project/Datasets/Cat & Dog/train'
full_dataset = ImageFolder(train_path, transform=transform)

# Split dataset into train and validation sets
train_size = int(0.8 * len(full_dataset))  # 80% for training
valid_size = len(full_dataset) - train_size  # 20% for validation

train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

# Rest of the code remains the same...
model = resnet18(weights=ResNet18_Weights.DEFAULT)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 2)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training function
def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=5):
    train_losses = []
    valid_losses = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        # Validation
        model.eval()
        valid_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                valid_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        train_losses.append(running_loss/len(train_loader))
        valid_losses.append(valid_loss/len(valid_loader))

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Training Loss: {train_losses[-1]:.4f}')
        print(f'Validation Loss: {valid_losses[-1]:.4f}')
        print(f'Validation Accuracy: {100 * correct / total:.2f}%\n')

    return train_losses, valid_losses

# Train the model
train_losses, valid_losses = train_model(model, train_loader, valid_loader, criterion, optimizer)

# Plot results
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Training Loss')
plt.plot(valid_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# Save model
torch.save(model.state_dict(), '/content/drive/MyDrive/cat_dog_classifier.pth')
#Creating the model for the cat-dog classifier

#Testing the cat-dog classifier

def load_model():
    # Load the model architecture
    model = resnet18(pretrained=False)
    num_features = model.fc.in_features
    model.fc = torch.nn.Linear(num_features, 2)

    # Load the trained weights
    model.load_state_dict(torch.load('/content/drive/MyDrive/cat_dog_classifier.pth'))
    model = model.to(device)
    model.eval()
    return model

def predict_image(image_path, model, show_image=True):
    # Load image from path or URL
    if image_path.startswith('http'):
        response = requests.get(image_path)
        image = Image.open(BytesIO(response.content))
    else:
        image = Image.open(image_path)

    # Display the image
    if show_image:
        plt.figure(figsize=(8, 8))
        plt.imshow(image)
        plt.axis('off')
        plt.show()

    # Preprocess the image
    image_tensor = transform(image).unsqueeze(0).to(device)

    # Get prediction
    with torch.no_grad():
        outputs = model(image_tensor)
        _, predicted = torch.max(outputs, 1)
        probability = torch.nn.functional.softmax(outputs, dim=1)

    # Get prediction class and probability
    classes = ['cat', 'dog']
    predicted_class = classes[predicted.item()]
    confidence = probability[0][predicted.item()].item() * 100

    print(f'Prediction: {predicted_class.upper()}')
    print(f'Confidence: {confidence:.2f}%')

    return predicted_class, confidence

model = load_model()
image_path = "/content/drive/MyDrive/Final Year Project/Datasets/Cat & Dog/test/dogs/dog_114.jpg"
predict_image(image_path, model)
#Testing the cat-dog classifier

import torch
import torchvision
import pandas as pd
import numpy as np
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, Subset
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import os
from google.colab import drive

# Custom Dataset class to handle images and annotations
class SkinDiseaseDataset(Dataset):
    def __init__(self, img_dir, annotation_file, transform=None):
        self.img_dir = img_dir
        self.annotations = pd.read_csv(annotation_file)
        self.transform = transform

        # Create class mapping
        self.classes = ['healthy', 'hypersensitivity-allergic-dermatosis',
                       'fungal-infection', 'bacterial-dermatosis']
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx]['filename'])
        image = Image.open(img_name).convert('RGB')
        label = self.class_to_idx[self.annotations.iloc[idx]['class']]

        if self.transform:
            image = self.transform(image)

        return image, label

# Data preprocessing with augmentation
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomAdjustSharpness(sharpness_factor=2),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Create datasets
def create_datasets(base_path):
    train_dataset = SkinDiseaseDataset(
        img_dir=os.path.join(base_path, 'train'),
        annotation_file=os.path.join(base_path, 'train', 'annotations.csv'),
        transform=train_transform
    )

    valid_dataset = SkinDiseaseDataset(
        img_dir=os.path.join(base_path, 'valid'),
        annotation_file=os.path.join(base_path, 'valid', 'annotations.csv'),
        transform=test_transform
    )

    test_dataset = SkinDiseaseDataset(
        img_dir=os.path.join(base_path, 'test'),
        annotation_file=os.path.join(base_path, 'test', 'annotations.csv'),
        transform=test_transform
    )

    return train_dataset, valid_dataset, test_dataset

# Create model
def create_model():
    model = resnet50(weights=ResNet50_Weights.DEFAULT)
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(num_features, 4)  # 4 disease classes
    )
    return model.to(device)

# Training function
def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=20):
    best_valid_loss = float('inf')
    train_losses = []
    valid_losses = []
    valid_accuracies = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        model.eval()
        valid_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                valid_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        # Calculate metrics
        epoch_train_loss = running_loss/len(train_loader)
        epoch_valid_loss = valid_loss/len(valid_loader)
        epoch_accuracy = 100 * correct / total

        train_losses.append(epoch_train_loss)
        valid_losses.append(epoch_valid_loss)
        valid_accuracies.append(epoch_accuracy)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'Training Loss: {epoch_train_loss:.4f}')
        print(f'Validation Loss: {epoch_valid_loss:.4f}')
        print(f'Validation Accuracy: {epoch_accuracy:.2f}%\n')

        # Save best model
        if epoch_valid_loss < best_valid_loss:
            best_valid_loss = epoch_valid_loss
            torch.save(model.state_dict(), '/content/drive/MyDrive/pet_disease_classifier.pth')

    return train_losses, valid_losses, valid_accuracies

# Plot training results
def plot_training_results(train_losses, valid_losses, valid_accuracies):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Plot losses
    ax1.plot(train_losses, label='Training Loss')
    ax1.plot(valid_losses, label='Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()

    # Plot accuracy
    ax2.plot(valid_accuracies, label='Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Validation Accuracy')
    ax2.legend()

    plt.show()

def create_subset(dataset, subset_ratio):
    """ Create a subset of the dataset based on the specified ratio. """
    dataset_size = len(dataset)
    subset_size = int(subset_ratio * dataset_size)
    indices = np.random.choice(range(dataset_size), subset_size, replace=False)
    return Subset(dataset, indices)

# Main training script
def main():
    base_path = '/content/drive/MyDrive/Final Year Project/Datasets/Dog Diseases'

    train_dataset, valid_dataset, test_dataset = create_datasets(base_path)

    train_subset = create_subset(train_dataset, subset_ratio=0.3)
    valid_subset = create_subset(valid_dataset, subset_ratio=0.3)
    test_subset = create_subset(test_dataset, subset_ratio=0.3)

    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_subset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)

    # Create model and training components
    model = create_model()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    train_losses, valid_losses, valid_accuracies = train_model(
        model, train_loader, valid_loader, criterion, optimizer
    )

    plot_training_results(train_losses, valid_losses, valid_accuracies)

    return model, train_dataset.classes

if __name__ == "__main__":
    model, classes = main()

#Testing for disease
def evaluate_model(model, test_loader, classes):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=classes))

def predict_single_image(image_path, model, classes):
    image = Image.open(image_path).convert('RGB')
    transform = test_transform

    # Display image
    plt.figure(figsize=(8, 8))
    plt.imshow(image)
    plt.axis('off')
    plt.show()

    # Predict
    image_tensor = transform(image).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        outputs = model(image_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        _, predicted = torch.max(outputs.data, 1)

    # Get prediction and probability
    predicted_class = classes[predicted.item()]
    confidence = probabilities[0][predicted.item()].item() * 100

    print(f'Predicted Disease: {predicted_class}')
    print(f'Confidence: {confidence:.2f}%')

    # Show all class probabilities
    print('\nProbabilities for all classes:')
    for i, prob in enumerate(probabilities[0]):
        print(f'{classes[i]}: {prob.item()*100:.2f}%')
#testing for disease

predict_single_image('/content/drive/MyDrive/Final Year Project/Datasets/Dog Diseases/test/pic4_jpg.rf.f4736c281de047486d83e03f3fc9148e.jpg', model, classes)

class Treatment:
    def __init__(self, medication, dosage, duration, additional_care=None):
        self.medication = medication
        self.dosage = dosage
        self.duration = duration
        self.additional_care = additional_care or []

class DiseaseTreatmentManager:
    def __init__(self):
        # Initialize disease-treatment mapping
        self.treatment_database = {
            "hypersensitivity_allergic_dermatosis": Treatment(
                medication="Antihistamine",
                dosage="1-2mg per kg body weight",
                duration="7-14 days",
                additional_care=[
                    "Avoid allergens",
                    "Use hypoallergenic shampoo",
                    "Regular grooming"
                ]
            ),
            "fungal_infection": Treatment(
                medication="Antifungal medication",
                dosage="Based on pet weight",
                duration="21-30 days",
                additional_care=[
                    "Isolate from other pets",
                    "Disinfect environment",
                    "Weekly medicated baths"
                ]
            ),
            "bacterial_dermatosis": Treatment(
                medication="Topical antibiotics",
                dosage="Apply twice daily",
                duration="7-10 days",
                additional_care=[
                    "Keep area clean and dry",
                    "Use E-collar if needed",
                    "Regular cleaning"
                ]
            ),
            "healthy": Treatment(
                medication="N/A",
                dosage="N/A",
                duration="N/A",
                additional_care=[
                    "Treat all pets in household",
                    "Treat environment"
                ]
            )
        }
    def get_treatment(self, disease_name):
        disease_name = disease_name.lower().replace(" ", "_")
        treatment = self.treatment_database.get(disease_name)

        if treatment:
            return {
                "disease": disease_name,
                "treatment_plan": {
                    "medication": treatment.medication,
                    "dosage": treatment.dosage,
                    "duration": treatment.duration,
                    "additional_care": treatment.additional_care
                }
            }
        else:
            return {
                "error": f"No treatment found for {disease_name}",
                "recommendation": "Please consult a veterinarian"
            }

    def add_disease_treatment(self, disease_name, medication, dosage, duration, additional_care=None):
        disease_name = disease_name.lower().replace(" ", "_")
        self.treatment_database[disease_name] = Treatment(
            medication=medication,
            dosage=dosage,
            duration=duration,
            additional_care=additional_care
        )

# Example usage with CNN integration
def process_skin_condition(image_prediction):
    treatment_manager = DiseaseTreatmentManager()
    return treatment_manager.get_treatment(image_prediction)

if __name__ == "__main__":
    # Initialize the treatment manager
    manager = DiseaseTreatmentManager()

    # Example: Process a CNN prediction
    # Simulate your CNN output
    cnn_prediction = "fungal_infection"

    # Get treatment recommendation
    treatment_plan = process_skin_condition(cnn_prediction)

    # Display results
    print("\nDisease Diagnosis:", cnn_prediction)
    print("\nTreatment Recommendation:")
    print("------------------------")
    if "error" not in treatment_plan:
        plan = treatment_plan["treatment_plan"]
        print(f"Medication: {plan['medication']}")
        print(f"Dosage: {plan['dosage']}")
        print(f"Duration: {plan['duration']}")
        print("\nAdditional Care Instructions:")
        for care in plan['additional_care']:
            print(f"- {care}")
    else:
        print(treatment_plan["error"])
        print(treatment_plan["recommendation"])